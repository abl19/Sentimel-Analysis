NLP: the process of understanding natural language to process and make decisions. 
-	Information Extraction - Get relevant information 

NLU: Natural language 

1. Tokenization: Tokenization is the process of converting a text into smaller sub-texts, based on certain predefined rules.
For example, sentences are tokenized to words (and punctuation optionally).
 And paragraphs into sentences, depending on the context. Tokenization is the next step after sentence detection.
 It allows you to identify the basic units in your text. These basic units are called tokens. 
Tokenization is useful because it breaks a text into meaningful units. 
These units are used for further analysis, like part of speech tagging.

2. Stops words, punctuation, and converting verbs into  gerund or base form 
Stop words are the most common words in a language. 
In the English language, some examples of stop words are the, are, but, and they.
 Most sentences need to contain stop words in order to be full sentences that make sense.
What is needed: 
			about_no_stopword_doc = [token for token in about_doc if not token.is_stop]

3. Vectorization text 
transform the token into a vector or numeric array, that is unique.
This is done by using .vector 

-----------------------
nlp object 




Documentation for Spacy:
1.pip install spacy 
2.python -m spacy download 
1. Import and install the library Spacy 
2. Create the English object that contains the rules for tokenization : method to create a blank English pipeline
It also includes language-specific rules used for tokenizing the text into words and punctuation. spaCy supports a variety of languages.
	nlp=spacy.blank('en')